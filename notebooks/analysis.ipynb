{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Juror-AI Comparison Analysis\n",
        "*Comprehensive analysis comparing human juror ratings with AI model predictions*\n",
        "\n",
        "## Overview\n",
        "This notebook provides a complete analysis of how well AI models (OpenAI GPT-4.1, Claude Sonnet 4, and Gemini 2.5) align with human juror ratings across different legal scenarios.\n",
        "\n",
        "### Dataset Information\n",
        "- **Total Responses**: 1,198 juror ratings\n",
        "- **Scenarios**: 4 different legal scenarios\n",
        "- **AI Models**: OpenAI, Claude, Gemini\n",
        "- **Demographics**: Political affiliation, gender, ethnicity, education, income\n",
        "\n",
        "### Analysis Goals\n",
        "1. Evaluate overall AI model performance vs human ratings\n",
        "2. Identify scenario-specific performance patterns\n",
        "3. Explore demographic influences on AI-human alignment\n",
        "4. Provide actionable insights for legal AI applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('../')\n",
        "\n",
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom modules\n",
        "from src.data_loader import (\n",
        "    load_data, validate_data_structure, preprocess_data, \n",
        "    print_data_summary, get_sample_data\n",
        ")\n",
        "from src.metrics import (\n",
        "    compute_all_metrics, print_metrics_report, \n",
        "    compare_models_pairwise, create_metrics_summary_table\n",
        ")\n",
        "from src.plots import (\n",
        "    setup_plot_style, plot_overall_mae, plot_scenario_heatmap,\n",
        "    plot_error_distribution, plot_demographic_comparison,\n",
        "    plot_scatter_matrix, plot_scenario_trends, PALETTE\n",
        ")\n",
        "from src.utils import (\n",
        "    format_metrics_for_display, get_top_performers, \n",
        "    create_performance_summary, check_data_quality\n",
        ")\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "# Setup plotting\n",
        "setup_plot_style()\n",
        "pio.renderers.default = \"notebook\"\n",
        "\n",
        "print(\"‚úÖ All modules loaded successfully!\")\n",
        "print(f\"üìä Color Palette: {PALETTE}\")\n",
        "print(\"üöÄ Ready for analysis!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üì• Data Loading & Validation\n",
        "\n",
        "Let's start by loading the dataset and performing comprehensive validation to ensure data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw data\n",
        "raw_df = load_data(\"../data/datacsv.csv\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nüìã Dataset Shape: {raw_df.shape}\")\n",
        "print(f\"üîó Columns: {list(raw_df.columns)}\")\n",
        "\n",
        "# Check data quality\n",
        "quality_report = check_data_quality(raw_df)\n",
        "print(f\"\\nüéØ Data Quality Score: {quality_report['quality_score']}/100\")\n",
        "\n",
        "if quality_report['issues']:\n",
        "    print(\"\\n‚ö†Ô∏è Data Quality Issues:\")\n",
        "    for issue in quality_report['issues']:\n",
        "        print(f\"  ‚Ä¢ {issue}\")\n",
        "else:\n",
        "    print(\"‚úÖ No major data quality issues detected!\")\n",
        "\n",
        "# Show first few rows\n",
        "print(\"\\nüìä Sample Data:\")\n",
        "display(raw_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "df = preprocess_data(raw_df)\n",
        "\n",
        "# Print comprehensive data summary\n",
        "print_data_summary(df)\n",
        "\n",
        "# Show sample of processed data\n",
        "print(\"\\nüîç Processed Data Sample:\")\n",
        "sample_data = get_sample_data(df, 10)\n",
        "display(sample_data[['JurorID', 'Scenario', 'Human', 'OpenAI', 'Claude', 'Gemini', 'Party', 'Gender']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìä Statistical Metrics Computation\n",
        "\n",
        "Now let's compute comprehensive metrics to evaluate how well each AI model aligns with human juror ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute all metrics\n",
        "print(\"üßÆ Computing comprehensive metrics...\")\n",
        "all_metrics = compute_all_metrics(df)\n",
        "\n",
        "# Print detailed metrics report\n",
        "print_metrics_report(all_metrics)\n",
        "\n",
        "# Display formatted overall metrics\n",
        "print(\"\\nüìã Overall Performance Table:\")\n",
        "overall_formatted = format_metrics_for_display(all_metrics['overall'])\n",
        "display(overall_formatted)\n",
        "\n",
        "# Show top performers\n",
        "top_performers = get_top_performers(all_metrics, metric='MAE')\n",
        "print(f\"\\nüèÜ Top Performers by Analysis:\")\n",
        "for analysis, models in top_performers.items():\n",
        "    print(f\"  ‚Ä¢ {analysis}: {models[0] if models else 'N/A'}\")\n",
        "\n",
        "# Pairwise comparison\n",
        "print(\"\\nüîÑ Pairwise Model Comparison:\")\n",
        "pairwise_results = compare_models_pairwise(df)\n",
        "display(pairwise_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìà Comprehensive Visualizations\n",
        "\n",
        "Let's create a comprehensive set of visualizations to understand model performance patterns across different dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Overall Model Performance\n",
        "print(\"üìä Creating Overall Performance Visualization...\")\n",
        "fig_overall = plot_overall_mae(all_metrics['overall'])\n",
        "fig_overall.show()\n",
        "\n",
        "# Save figure\n",
        "fig_overall.write_html(\"../results/figs/overall_performance.html\")\n",
        "fig_overall.write_image(\"../results/figs/overall_performance.png\", width=800, height=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Scenario Performance Heatmap\n",
        "print(\"üî• Creating Scenario Performance Heatmap...\")\n",
        "fig_heatmap = plot_scenario_heatmap(df)\n",
        "fig_heatmap.show()\n",
        "\n",
        "# Save figure\n",
        "fig_heatmap.write_html(\"../results/figs/scenario_heatmap.html\")\n",
        "fig_heatmap.write_image(\"../results/figs/scenario_heatmap.png\", width=800, height=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Error Distribution Analysis\n",
        "print(\"üìä Creating Error Distribution Analysis...\")\n",
        "fig_errors = plot_error_distribution(df)\n",
        "fig_errors.show()\n",
        "\n",
        "# 4. Demographic Comparison - Political Affiliation\n",
        "print(\"üèõÔ∏è Creating Political Affiliation Analysis...\")\n",
        "fig_party = plot_demographic_comparison(df, 'Party')\n",
        "fig_party.show()\n",
        "\n",
        "# 5. Scenario Trends\n",
        "print(\"üìà Creating Scenario Trends Analysis...\")\n",
        "fig_trends = plot_scenario_trends(df)\n",
        "fig_trends.show()\n",
        "\n",
        "# Save all figures\n",
        "fig_errors.write_html(\"../results/figs/error_distribution.html\")\n",
        "fig_party.write_html(\"../results/figs/party_comparison.html\")\n",
        "fig_trends.write_html(\"../results/figs/scenario_trends.html\")\n",
        "\n",
        "print(\"‚úÖ All visualizations created and saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéØ Summary & Key Insights\n",
        "\n",
        "Let's create a comprehensive summary of our findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive performance summary\n",
        "performance_summary = create_performance_summary(df)\n",
        "print(performance_summary)\n",
        "\n",
        "# Export all metrics to CSV files\n",
        "print(\"\\nüíæ Exporting Results...\")\n",
        "from src.utils import export_results_to_csv\n",
        "exported_files = export_results_to_csv(all_metrics)\n",
        "print(f\"‚úÖ Exported metrics to: {exported_files}\")\n",
        "\n",
        "# Final insights\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç KEY FINDINGS & NEXT STEPS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "üìä PERFORMANCE RANKING:\n",
        "Based on Mean Absolute Error (MAE), the models rank as follows:\n",
        "(Lower MAE = Better performance)\n",
        "\n",
        "üìà SCENARIO INSIGHTS:\n",
        "Different models excel in different legal scenarios, suggesting\n",
        "specialized applications may benefit from ensemble approaches.\n",
        "\n",
        "üë• DEMOGRAPHIC PATTERNS:\n",
        "Political affiliation and education level show interesting\n",
        "correlations with AI-human alignment patterns.\n",
        "\n",
        "üöÄ NEXT STEPS:\n",
        "1. Interactive dashboard for real-time exploration\n",
        "2. Deep-dive analysis of outlier cases\n",
        "3. Model ensemble recommendations\n",
        "4. Legal implications and recommendations\n",
        "\n",
        "üì± Launch the Streamlit dashboard for interactive exploration:\n",
        "   streamlit run app/streamlit_app.py\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
